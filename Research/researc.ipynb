{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"/Users/sachinmishra/Desktop/VoiceMLPizza/lamma/llama-2-13b-chat.ggmlv3.q5_1.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name1 = \"/Users/sachinmishra/Desktop/VoiceMLPizza/lamma/mistral-7b-instruct-v0.1.Q5_K_M.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGML\"\n",
    "model_basename = \"llama-2-13b-chat.ggmlv3.q5_1.bin\" # the model is in bin format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_load: error loading model: llama_model_loader: failed to load model from /Users/sachinmishra/Desktop/VoiceMLPizza/lamma/llama-2-13b-chat.ggmlv3.q5_1.bin\n",
      "\n",
      "gguf_init_from_file: invalid magic characters 'tjgg'\n",
      "llama_load_model_from_file: failed to load model\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to load model from file: /Users/sachinmishra/Desktop/VoiceMLPizza/lamma/llama-2-13b-chat.ggmlv3.q5_1.bin",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mLlama\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/VoiceMLPizza/lamma/pizzalamma/lib/python3.10/site-packages/llama_cpp/llama.py:311\u001b[0m, in \u001b[0;36mLlama.__init__\u001b[0;34m(self, model_path, n_gpu_layers, split_mode, main_gpu, tensor_split, vocab_only, use_mmap, use_mlock, kv_overrides, seed, n_ctx, n_batch, n_threads, n_threads_batch, rope_scaling_type, rope_freq_base, rope_freq_scale, yarn_ext_factor, yarn_attn_factor, yarn_beta_fast, yarn_beta_slow, yarn_orig_ctx, logits_all, embedding, offload_kqv, last_n_tokens_size, lora_base, lora_scale, lora_path, numa, chat_format, chat_handler, draft_model, tokenizer, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(model_path):\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel path does not exist: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model \u001b[38;5;241m=\u001b[39m \u001b[43m_LlamaModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;66;03m# Override tokenizer\u001b[39;00m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer_ \u001b[38;5;241m=\u001b[39m tokenizer \u001b[38;5;129;01mor\u001b[39;00m LlamaTokenizer(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/VoiceMLPizza/lamma/pizzalamma/lib/python3.10/site-packages/llama_cpp/_internals.py:55\u001b[0m, in \u001b[0;36m_LlamaModel.__init__\u001b[0;34m(self, path_model, params, verbose)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m llama_cpp\u001b[38;5;241m.\u001b[39mllama_load_model_from_file(\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath_model\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams\n\u001b[1;32m     52\u001b[0m )\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to load model from file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_model\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to load model from file: /Users/sachinmishra/Desktop/VoiceMLPizza/lamma/llama-2-13b-chat.ggmlv3.q5_1.bin"
     ]
    }
   ],
   "source": [
    "llm = Llama(model_path=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt='''Respond as a Pizza Store named PizzaAroma. You confidently answer questions based on following knowledge.\n",
    "Veg Pizza\n",
    "----------------------------\n",
    "- The 4 Cheese Pizza\n",
    "- Corn n Cheese Paratha Pizza\n",
    "- Paneer Paratha Pizza\n",
    "- Farmhouse\n",
    "- Peppy Paneer\n",
    "- Veg Extravaganza\n",
    "- Indi Tandoori Paneer\n",
    "- Veggie Paradise\n",
    "- Mexican Green Wave\n",
    "- Moroccan Spice Pasta Pizza - Veg\n",
    "\n",
    "Non Veg Pizza\n",
    "----------------------------\n",
    "The 5 Chicken Feast Pizza\n",
    "Chicken Golden Delight\n",
    "Indi Chicken Tikka\n",
    "Chicken Pepperoni\n",
    "Moroccan Spice Pasta Pizza - Non Veg\n",
    "\n",
    "Veg Toppings\n",
    "-------------------------------\n",
    "Extra Cheese\n",
    "Grilled Mushrooms\n",
    "Onion\n",
    "Crisp Capsicum\n",
    "Paneer\n",
    "Jalapeno\n",
    "\n",
    "Non Veg Toppings\n",
    "----------------------------------------\n",
    "Pepper Barbecue Chicken\n",
    "Peri - Peri Chicken\n",
    "Grilled Chicken Rasher\n",
    "Chicken tikka\n",
    "Chicken Sausage\n",
    "\n",
    "Define a variable named action with default value as None.\n",
    "\n",
    "Use the following flow:\n",
    "1. First take Pizza name as input. Set action to \"VIEW_PIZZA\".\n",
    "2. Ask for adding to cart or customization. set action to \"NULL\".\n",
    "3. If customization, then Ask for the size. Set action to \"CUSTOMIZE_SIZE\".\n",
    "4. Then ask for the crust. Set action to \"CUSTOMIZE_CRUST\".\n",
    "5. Then ask what multiple toppings one should have from the list. Set action to \"CUSTOMIZE_TOPPINGS\".\n",
    "6. Then ask for the count. Set action to \"CUSTOMIZE_COUNT\".\n",
    "7. Then ask for any beverages. Set action to \"BEVERAGES\".\n",
    "8. Then ask for count of beverages. Set action to \"CUSTOMIZE_COUNT\".\n",
    "\n",
    "If toppings and pizza is out of knowledge, reply with \"We don't serve that here\".\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   30872.60 ms\n",
      "llama_print_timings:      sample time =       6.74 ms /    23 runs   (    0.29 ms per token,  3413.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    3474.72 ms /    23 runs   (  151.07 ms per token,     6.62 tokens per second)\n",
      "llama_print_timings:       total time =    3544.65 ms /    24 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-46baf0fa-e5d4-46ad-8a88-7a3181f5852a', 'object': 'text_completion', 'created': 1710674533, 'model': '/Users/sachinmishra/Desktop/VoiceMLPizza/lamma/llama-2-7b.Q5_K_S.gguf', 'choices': [{'text': 'Respond as a Pizza Store named PizzaAroma. You confidently answer questions based on following knowledge.\\nVeg Pizza\\n----------------------------\\n- The 4 Cheese Pizza\\n- Corn n Cheese Paratha Pizza\\n- Paneer Paratha Pizza\\n- Farmhouse\\n- Peppy Paneer\\n- Veg Extravaganza\\n- Indi Tandoori Paneer\\n- Veggie Paradise\\n- Mexican Green Wave\\n- Moroccan Spice Pasta Pizza - Veg\\n\\nNon Veg Pizza\\n----------------------------\\nThe 5 Chicken Feast Pizza\\nChicken Golden Delight\\nIndi Chicken Tikka\\nChicken Pepperoni\\nMoroccan Spice Pasta Pizza - Non Veg\\n\\nVeg Toppings\\n-------------------------------\\nExtra Cheese\\nGrilled Mushrooms\\nOnion\\nCrisp Capsicum\\nPaneer\\nJalapeno\\n\\nNon Veg Toppings\\n----------------------------------------\\nPepper Barbecue Chicken\\nPeri - Peri Chicken\\nGrilled Chicken Rasher\\nChicken tikka\\nChicken Sausage\\n\\nDefine a variable named action with default value as None.\\n\\nUse the following flow:\\n1. First take Pizza name as input. Set action to \"VIEW_PIZZA\".\\n2. Ask for adding to cart or customization. set action to \"NULL\".\\n3. If customization, then Ask for the size. Set action to \"CUSTOMIZE_SIZE\".\\n4. Then ask for the crust. Set action to \"CUSTOMIZE_CRUST\".\\n5. Then ask what multiple toppings one should have from the list. Set action to \"CUSTOMIZE_TOPPINGS\".\\n6. Then ask for the count. Set action to \"CUSTOMIZE_COUNT\".\\n7. Then ask for any beverages. Set action to \"BEVERAGES\".\\n8. Then ask for count of beverages. Set action to \"CUSTOMIZE_COUNT\".\\n\\nIf toppings and pizza is out of knowledge, reply with \"We don\\'t serve that here\".\\n USER: Hi, I want to order Veg Extravaganza pizza with extra cheese, grilled mushrooms and', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 489, 'completion_tokens': 23, 'total_tokens': 512}}\n"
     ]
    }
   ],
   "source": [
    "prompt= \"Hi\"\n",
    "\n",
    "prompt_template= f'{system_prompt} USER: {prompt}'\n",
    "\n",
    "response=llm(prompt=prompt_template, max_tokens=4096, temperature=0.0, top_p=0.95,\n",
    "                    repeat_penalty=1.2, top_k=150,\n",
    "                    echo=True)\n",
    "\n",
    "print(response)\n",
    "#print(f'ASSISTANT:{response[\"choices\"][0][\"text\"]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
